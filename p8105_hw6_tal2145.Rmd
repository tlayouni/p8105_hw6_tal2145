---
title: "Homework 6"
author: Troy Layouni
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(modelr)
```

## Problem 1
***

#### Part 1: Loading and cleaning `birthweight` dataset

* Loading `birthweight.csv` 
* cleaning variable types and recoding values to meaningful names
* checked for missing data

```{r}
birthweight_df = 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    mrace = as.factor(mrace), 
    mrace = recode(mrace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
    frace = as.factor(frace), 
    frace = recode(frace, "1" = "White", "2" = "Black", "3" = "Asian", "4" = "Puerto Rican", "8" = "Other", "9" = "Unknown"),
    malform = as.factor(malform),
    malform = recode(malform, "1" = "absent", "2" = "absent"),
    babysex = as.factor(babysex),
    babysex = recode(babysex, "1" = "male", "2" = "female")
  )

colSums(is.na(birthweight_df))
```

#### Part 2: Building a linear model

Proposed linear model: 

* this model was created through a data-driven process, adding in variables one at a time, to see whether they were significant and whether the adjusted r-squared changed by an appreciable amount to determine whether the variable should be included in my regression model. gaweeks was always included as it is a key factor in an infant's birthweight.

```{r}
birthweight_df = 
  birthweight_df %>% 
  mutate(
    mrace = fct_infreq(mrace)
  )

fit = lm(bwt ~ bhead + blength + delwt + mrace + gaweeks, data = birthweight_df)

summary(fit)
```

#### Part 3: Plotting the residuals against the fitted values for the hypothesized model

```{r}
birthweight_df %>% 
  modelr::add_residuals(fit) %>% 
  modelr::add_predictions(fit) %>% 
  ggplot(aes(x = pred, y = resid)) + 
  geom_point() + 
  labs(
    title = "Fitted Values for Birthweight Regression Model compared to Residuals",
    x = "Predicted Values",
    y = "Model Residuals"
  )
```

#### Part 4: Comparing the hypothesized model to two additional models for birthweight using cross-validation

*  using `crossv_mc` and `purr` functions to compare prediction error between the three models


**Using `crossv_mc` to perform the training/testing split and storing the data as a dataframe**

```{r}
cv_df = 
  crossv_mc(birthweight_df, 4342)

cv_df =
  cv_df %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble))
```


**Fitting models (hypothesized and two comparisons) and assessing prediction accuracy**

```{r}
cv_df = 
  cv_df %>% 
  mutate(
    fit  = map(train, ~lm(bwt ~ bhead + blength + delwt + mrace + gaweeks, data = birthweight_df)),
         fit_2  = map(train, ~lm(bwt ~ gaweeks + blength, data = birthweight_df)),
         fit_3  = map(train, ~lm(bwt ~ bhead + babysex + blength + bhead * babysex * blength, data = birthweight_df)) 
  ) %>% 
  mutate(rmse_fit = map2_dbl(fit, test, ~rmse(model = .x, data = .y)),
         rmse_fit_2 = map2_dbl(fit_2, test, ~rmse(model = .x, data = .y)),
         rmse_fit_3 = map2_dbl(fit_3, test, ~rmse(model = .x, data = .y)))

```

**Plotting and comparing RMSE between the three models**

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin() + 
  labs(
    title = "Comparing RMSE between three hypothesized models",
    x = "Model",
    y = "RMSE"
  ) 
```

Based on the above plot showing the variance in prediction error distribution across the three models, it appears the hypothesized model that I created is optimal, but only slightly better than the model that contains head circumference, length, sex and all interaction terms between them. We can see this because `fit` or my hypothesized model, has the lowest RMSE distribution of all three models and this means it is a better predictor of birthweight in this sample. 


## Problem 2
***

#### Part 1: Loading in the Central Park Weather data for 2017

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

#### Part 1: Drawing boostrapping samples from `weather_df`

* drawing 5000 bootstrapped samples
* creating log(B1*B0) variable

```{r}
set.seed(1)

weather_bootstrapped = 
  weather_df %>% 
  modelr::bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x)),
    betas = map(models, broom::tidy),
    ) %>% 
  select(-strap) %>% 
  unnest(betas) %>% 
  select(models, term, estimate) %>% 
  mutate(r2 = map(models, broom::glance)) %>% 
  unnest(r2) %>% 
  select(term, estimate, r.squared) %>%
    pivot_wider(
      names_from = "term", 
      values_from = "estimate"
    ) %>% 
  rename(
    "beta0" = "(Intercept)",
    "beta1" = "tmin"
  ) %>% 
  mutate(
    product_logb = log(beta0*beta1)
  )
```

#### Plotting distribution of r-squared and product of the betas. 

**Plotting log (β1*β2) Distribution**

```{r}
weather_bootstrapped %>% 
  ggplot(aes(x = product_logb)) +
  geom_density() +
  labs(
    title = "Distribution of log (β1*β2)",
    x = "log (β1*β2)"
  )
```

The distribution of log (β1*β2) for the 5000 boostrapped samples appears to be normally distributed and symmetric, with a peak around 2.02 or so and most of the data between 1.975 and 2.05.  

**Plotting log R-squared Distribution**

```{r}
weather_bootstrapped %>% 
  ggplot(aes(x = r.squared)) +
  geom_density() +
  labs(
    title = "Distribution of R-squared",
    x = "R-squared"
  )
```

The distribution for the r-squared values of our 5000 bootstrapped samples has a peak around 1.93 or so with most values between 0.89 and 0.93. While it appears to be normally distributed, it looks like it is slightly left-skewed, with a longer tail on the left side of the distribution. 

#### 95% Confidence Intervals for R-squared 

```{r}
alpha = 0.05

weather_bootstrapped %>% 
  summarize(
    lower_CI = quantile(r.squared, alpha / 2),
    upper_CI = quantile(r.squared, 1 - alpha / 2)
  )
```

The 95% Confidence interval for the r-squared in our bootstrapped samples is 0.894 to 0.927. 


#### 95% Confidence Intervals for log (β1*β2)

```{r}
alpha = 0.05

weather_bootstrapped %>% 
  summarize(
    lower_CI = quantile(product_logb, alpha / 2),
    upper_CI = quantile(product_logb, 1 - alpha / 2)
  )
```

The 95% Confidence interval for log (β1*β2) in our bootstrapped samples is 1.96 to 2.06. 



